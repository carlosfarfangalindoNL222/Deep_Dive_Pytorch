{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.DoubleTensor'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "print(torch.__version__)  \n",
    "\n",
    "# Data from House Prices: Advanced Regression Techniques\n",
    "# Cleaned Data\n",
    "X_train_pd = pd.read_csv('X_train_HP.csv')\n",
    "y_train_pd = pd.read_csv('y_train_HP.csv')\n",
    "X_cv_pd = pd.read_csv('X_cv_HP.csv')\n",
    "y_cv_pd = pd.read_csv('y_cv_HP.csv')\n",
    "test_pd = pd.read_csv('test_skew_HP.csv')\n",
    "\n",
    "# Convert to tensor and check Datatype\n",
    "\n",
    "X_t = torch.from_numpy(X_train_pd.values)\n",
    "y_t = torch.from_numpy(y_train_pd.values)\n",
    "X_cv = torch.from_numpy(X_cv_pd.values)\n",
    "y_cv = torch.from_numpy(y_cv_pd.values)\n",
    "test = torch.from_numpy(test_pd.values)\n",
    "\n",
    "X_t.type()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "*(Linear Regression with PyTorch)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1022/1022], Train Loss: 109.8790, CV Loss: 123.6562\n",
      "Epoch [2/200], Step [1022/1022], Train Loss: 67.5519, CV Loss: 71.2983\n",
      "Epoch [3/200], Step [1022/1022], Train Loss: 15.0009, CV Loss: 15.2063\n",
      "Epoch [4/200], Step [1022/1022], Train Loss: 0.0781, CV Loss: 0.7028\n",
      "Epoch [5/200], Step [1022/1022], Train Loss: 2.6009, CV Loss: 0.3514\n",
      "Epoch [6/200], Step [1022/1022], Train Loss: 0.1568, CV Loss: 0.3227\n",
      "Epoch [7/200], Step [1022/1022], Train Loss: 0.0094, CV Loss: 0.2970\n",
      "Epoch [8/200], Step [1022/1022], Train Loss: 0.9490, CV Loss: 0.2658\n",
      "Epoch [9/200], Step [1022/1022], Train Loss: 0.2704, CV Loss: 0.2401\n",
      "Epoch [10/200], Step [1022/1022], Train Loss: 0.0808, CV Loss: 0.2144\n",
      "Epoch [11/200], Step [1022/1022], Train Loss: 0.4961, CV Loss: 0.1860\n",
      "Epoch [12/200], Step [1022/1022], Train Loss: 0.0130, CV Loss: 0.1584\n",
      "Epoch [13/200], Step [1022/1022], Train Loss: 0.0444, CV Loss: 0.1528\n",
      "Epoch [14/200], Step [1022/1022], Train Loss: 0.0001, CV Loss: 0.1290\n",
      "Epoch [15/200], Step [1022/1022], Train Loss: 0.0817, CV Loss: 0.1137\n",
      "Epoch [16/200], Step [1022/1022], Train Loss: 0.0424, CV Loss: 0.0990\n",
      "Epoch [17/200], Step [1022/1022], Train Loss: 0.0378, CV Loss: 0.0944\n",
      "Epoch [18/200], Step [1022/1022], Train Loss: 0.0003, CV Loss: 0.0797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Save the Model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel_ab3.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m Abstract_1(X_t, y_t, X_cv, y_cv, test, \u001b[39m1\u001b[39;49m, \u001b[39m200\u001b[39;49m, \u001b[39m0.00001\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m, in \u001b[0;36mAbstract_1\u001b[0;34m(X, y, X_cv, y_cv, test, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     38\u001b[0m \u001b[39m# Print Loss and CV Loss every 100 epochs\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:209\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m grads\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m--> 209\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate[p]\n\u001b[1;32m    210\u001b[0m \u001b[39m# Lazy state initialization\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(state) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    929\u001b[0m     \u001b[39m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[39m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[39m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[39m# subclass if needed.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mid\u001b[39;49m(\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def Abstract_1(X, y, X_cv, y_cv, test, epochs, lr):\n",
    "    # Model\n",
    "    model = nn.Sequential(\n",
    "        # Input Layer\n",
    "        nn.Linear(X.shape[1], 32),\n",
    "        nn.ReLU(),\n",
    "        # Hidden Layers\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        # Output Layer\n",
    "        nn.Linear(16, 1),\n",
    "    )\n",
    "    # Loss and Optimizer with L2 Regularization\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.004)\n",
    "\n",
    "    # Train the Model\n",
    "    train_loss = []\n",
    "    cv_loss = []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X.float())\n",
    "        outputs_cv = model(X_cv.float())\n",
    "        loss = criterion(outputs, y.float())\n",
    "        loss_cv = criterion(outputs_cv, y_cv.float())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss values\n",
    "        train_loss.append(loss.item())\n",
    "        cv_loss.append(loss_cv.item())\n",
    "\n",
    "        # Print Loss and CV Loss\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Train Loss: %.4f, CV Loss: %.4f' \n",
    "                   %(epoch+1, epochs, loss.item(), loss_cv.item()))\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(cv_loss, label='Cross-Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, epochs)\n",
    "    plt.ylim(0, 0.1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the Model\n",
    "    torch.save(model.state_dict(), 'model_ab3.pkl')\n",
    "\n",
    "# Call the function\n",
    "Abstract_1(X_t, y_t, X_cv, y_cv, test, 20000, 0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.6151],\n",
      "        [11.9223],\n",
      "        [12.1279],\n",
      "        ...,\n",
      "        [12.0634],\n",
      "        [11.7053],\n",
      "        [12.2495]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pred_ab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[110757.51]\n",
      " [150579.28]\n",
      " [184954.33]\n",
      " ...\n",
      " [173404.97]\n",
      " [121211.25]\n",
      " [208871.3 ]]\n"
     ]
    }
   ],
   "source": [
    "# convert from 1logp format to normal p format\n",
    "pred_ab1 = np.exp(pred_ab1.detach().numpy()) - 1\n",
    "print(pred_ab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv file for submission\n",
    "sub_df = pd.read_csv('sample_submission.csv', index_col = \"Id\")\n",
    "sub_df[\"SalePrice\"] = pred_ab1\n",
    "sub_df.to_csv('submission_ab1_5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
